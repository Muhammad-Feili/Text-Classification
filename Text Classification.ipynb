{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"in this cell we divide the data set into two part:\n",
    "training set and test set;\n",
    "that training_set is two third of main data set and rest one third is for test_set.\"\"\"\n",
    "\n",
    "#loading the data set file\n",
    "import arff\n",
    "data=arff.load(open('dataMining.arff'))\n",
    "data=data['data']\n",
    "\n",
    "\n",
    "classes={}\n",
    "\n",
    "for d in data:\n",
    "    if d[0] in classes:\n",
    "        classes[d[0]]+=1\n",
    "    else:\n",
    "        classes[d[0]]=1\n",
    "        \n",
    "#declaring the sets       \n",
    "training_set=[]\n",
    "test_set=[]\n",
    "\n",
    "#computing algorithm\n",
    "for key in classes.keys():\n",
    "    t=classes[key]*(2/3)\n",
    "    i=0\n",
    "    for d in data:\n",
    "        if d[0]==key and i<t:\n",
    "            training_set.append(d)\n",
    "            i+=1\n",
    "        elif d[0]==key:\n",
    "            test_set.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramFiles\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aren', 'can', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'let', 'll', 'mustn', 're', 'shan', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "#get stop words for four languages\n",
    "english=get_stop_words('english')\n",
    "french=get_stop_words('french')\n",
    "german=get_stop_words('german')\n",
    "spanish=get_stop_words('spanish')\n",
    "\n",
    "#union then into one list\n",
    "stop_words=english+french+german+spanish\n",
    "\n",
    "#normalizing and integrating the data\n",
    "vect=TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "#separate the texts in training data set\n",
    "training_labels=[]\n",
    "training_texts=[]\n",
    "\n",
    "for L in training_set:\n",
    "    training_labels.append(L[0])\n",
    "    training_texts.append(L[1])\n",
    "    \n",
    "#separate the texts in test data set    \n",
    "test_labels=[]\n",
    "test_texts=[]\n",
    "\n",
    "for L in test_set:\n",
    "    test_labels.append(L[0])\n",
    "    test_texts.append(L[1])\n",
    "    \n",
    "#dtm: document term matrix\n",
    "#matrix of feature extracted training_set  \n",
    "training_dtm=vect.fit(training_texts)\n",
    "training_dtm=vect.transform(training_texts)\n",
    "training_term_matrix=pd.DataFrame(training_dtm.toarray(),columns=vect.get_feature_names())\n",
    "\n",
    "#matrix of feature extracted test_set\n",
    "test_dtm=vect.fit(test_texts)\n",
    "test_dtm=vect.transform(test_texts)\n",
    "test_term_matrix=pd.DataFrame(test_dtm.toarray(),columns=vect.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "\n",
    "data=pd.read_csv('dataMining.csv')\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(data['content'],data['class'],test_size=float(1/3),random_state=100)\n",
    "\n",
    "\n",
    "#training with using gini index\n",
    "\n",
    "train_gini=DecisionTreeClassifier(criterion='gini',max_depth=2)\n",
    "pl=Pipeline([('vect',TfidfVectorizer()),('tdidf',TfidfTransformer()),('clf',train_gini)])\n",
    "pl.fit(X_train,Y_train)\n",
    "test_pred_gini=pl.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.746031746031746\n",
      "[[ 9  0  0  0]\n",
      " [ 0  2  0  0]\n",
      " [ 1  5 21  4]\n",
      " [ 0  6  0 15]]\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(test_pred_gini,Y_test))\n",
    "print(confusion_matrix(test_pred_gini,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161     german\n",
      "77      french\n",
      "11     english\n",
      "158     german\n",
      "164     german\n",
      "160     german\n",
      "115    spanish\n",
      "157     german\n",
      "126    spanish\n",
      "15     english\n",
      "142     german\n",
      "117    spanish\n",
      "75      french\n",
      "92     spanish\n",
      "124    spanish\n",
      "152     german\n",
      "82      french\n",
      "119    spanish\n",
      "96     spanish\n",
      "46      french\n",
      "110    spanish\n",
      "146     german\n",
      "116    spanish\n",
      "88     spanish\n",
      "145     german\n",
      "85     spanish\n",
      "32     english\n",
      "122    spanish\n",
      "83      french\n",
      "7      english\n",
      "        ...   \n",
      "172     german\n",
      "168     german\n",
      "99     spanish\n",
      "185     german\n",
      "41      french\n",
      "1      english\n",
      "64      french\n",
      "127    spanish\n",
      "151     german\n",
      "59      french\n",
      "114    spanish\n",
      "175     german\n",
      "26     english\n",
      "163     german\n",
      "73      french\n",
      "149     german\n",
      "150     german\n",
      "40     english\n",
      "101    spanish\n",
      "104    spanish\n",
      "44      french\n",
      "28     english\n",
      "29     english\n",
      "139    spanish\n",
      "165     german\n",
      "65      french\n",
      "74      french\n",
      "89     spanish\n",
      "81      french\n",
      "188     german\n",
      "Name: class, Length: 63, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
